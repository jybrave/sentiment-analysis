{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = open('reviews.txt','r') # What we know!\n",
    "reviews = list(map(lambda x:x[:-1],g.readlines()))\n",
    "g.close()\n",
    "\n",
    "g = open('labels.txt','r') # What we WANT to know!\n",
    "labels = list(map(lambda x:x[:-1].upper(),g.readlines()))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   ', 'story of a man who has unnatural feelings for a pig . starts out with a opening scene that is a terrific example of absurd comedy . a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it  s singers . unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting . even those from the era should be turned off . the cryptic dialogue would make shakespeare seem easy to a third grader . on a technical level it  s better than you might think with some good cinematography by future great vilmos zsigmond . future stars sally kirkland and frederic forrest can be seen briefly .  ']\n",
      "<class 'list'>\n",
      "['POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE', 'POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "print(reviews[:2])\n",
    "print(type(reviews))\n",
    "print(labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reviews_split = []\n",
    "# all_text = []\n",
    "# for review in reviews:\n",
    "#     for c in review:\n",
    "#         all_text.extend([c for c in review if c not in punctuation])\n",
    "#         reviews_split.append([c for c in review if c not in punctuation])\n",
    "# print(all_text[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n"
     ]
    }
   ],
   "source": [
    "reviews_split = reviews\n",
    "print(reviews_split[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(reviews_split)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "print(len(reviews_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6347388\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life']\n"
     ]
    }
   ],
   "source": [
    "print(words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = Counter(words)\n",
    "reviews_ints = []\n",
    "for review in reviews_split:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  74073\n",
      "Tokenized review: \n",
      " [[8, 2161, 107328, 163009, 545, 3246, 327192, 96352, 238, 23513, 336713, 4053, 12724, 46933, 15747, 9163, 66, 17374, 1659, 6628, 5134, 46933, 77, 327192, 12503, 4517, 93968, 336713, 82, 65, 1310, 10773, 135720, 2505, 73245, 8, 2161, 65361, 261, 107328, 9763, 206, 135720, 987, 9919, 107328, 77, 327192, 336713, 6, 135720, 260, 29, 336713, 66, 361, 21433, 14654, 11478, 3313, 4969, 11385, 468, 77, 9, 336713, 2, 145864, 336713, 3078, 669, 23978, 157, 10773, 145864, 336713, 66, 87623, 898, 164107, 11385, 361, 327192, 14182, 87623, 3167, 336713, 1658, 93968, 12047, 163009, 392, 119, 773, 135720, 127, 3728, 336713, 1659, 87623, 462, 18, 327192, 327192, 327192, 327192, 327192, 327192, 327192, 327192, 327192, 23513, 327192, 327192, 327192, 327192, 327192, 327192, 327192, 327192, 327192, 327192, 2161, 327192, 163009, 1829, 1868, 162, 87623, 4998, 5767, 135720, 44, 26789, 145864, 5686, 77, 327192, 392, 214, 135720, 8, 2161, 327192, 87623, 1178, 73245, 6675, 376, 145864, 12503, 1119, 7298, 73245, 8, 2161, 107328, 2977, 103, 327192, 16159, 163009, 229, 73245, 96352, 3177, 34081]]\n"
     ]
    }
   ],
   "source": [
    "print('Unique words: ', len((vocab_to_int)))\n",
    "#打印第一条影评的数字编码\n",
    "print('Tokenized review: \\n', reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_labels = np.array([1 if label=='POSITIVE' else 0 for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使影评保持标准形状，还要执行一个预处理步骤。网络要求输入文本是标准大小，所以我们需要将影评变形为特定的长度。为了满足该要求，我们将完成两大步骤：\n",
    "\n",
    "1.删除超长或超短的影评，即离群值 2.填充或截断剩余数据，使所有影评长度一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 2633\n"
     ]
    }
   ],
   "source": [
    "review_lens = Counter([len(x) for x in reviews_ints])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063\n"
     ]
    }
   ],
   "source": [
    "print(len(review_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews before removing outliers:  25000\n",
      "Number of reviews after removing outliers:  25000\n"
     ]
    }
   ],
   "source": [
    "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
    "\n",
    "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
    "\n",
    "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of reviews after removing outliers: ', len(reviews_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于很短和很长的影评，我们将通过填充或截断方式使影评保持特定长度。对于短于 seq_length 的影评，我们将用 0 填充它。对于长于 seq_length 的影评，我们将截取前 seq_length 个字词。\n",
    "最终 features 数组应该为二维数组，行数等于影评数，列数等于指定的 seq_length。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    features=np.zeros([len(reviews_ints),seq_length],dtype = int)#先初始化特征\n",
    "    \n",
    "    for i,review in enumerate(reviews_ints):\n",
    "        features[i,-len(review):] = np.array(review)[:seq_length]#很好的赋值方法\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n",
      "[[  2977    103 327192  16159 163009    229  73245  96352   3177  34081]\n",
      " [   135     24 164107     13     42  14654  26957   6679    136 327192]\n",
      " [336713      4  16803     23    167  30138  14654   9177  16159  30138]\n",
      " [327192 101872 101872   9158   1079   1368 336713    541   2382      4]\n",
      " [  2580  12724  87623  11478  96352  12047 107328   3739   1601 327192]\n",
      " [ 48208    222  44343  21560    861  20617    279  44343    989 327192]\n",
      " [  4445 164107   1152  30626  20617   9763  46933 163009   1310 327192]\n",
      " [  1110  22906   1246    983 145864  76000 107328  10659    146 327192]\n",
      " [ 42603      2 164107     53   3406   2588  11385   1191  10659 327192]\n",
      " [ 40155 107328    240   1132 164107    816 327192  42603    866  73245]\n",
      " [327192  20498 327192 327192 327192 101872 101872 336713     43 327192]\n",
      " [    10 327192  87623  12436  27731 135720    572  44125  73245 327192]\n",
      " [  2483 135720     93   4971 164107    734 163009    157   1866  11988]\n",
      " [327192   9763 145864 336713    769 107328    995  20276 336713   4289]\n",
      " [  2175    178    711  46933 163009   1718 327192 163009   5383  18004]\n",
      " [ 44343 336713    786  42603  44343 336713   4484  46933  10659 327192]\n",
      " [ 18421     10  44125  12717  26789    183  73245     80 107328    314]\n",
      " [ 65361    417 327192    214  73245    792     52   5739    274 327192]\n",
      " [327192 101872 101872     60     56   2842   1577  18421   6413    667]\n",
      " [ 20498     85     11  17374  16159  48208  30138   1179  87623   7298]\n",
      " [ 73245   9045 336713    720   2228   9211   2842   7298    248    344]\n",
      " [   106  30138  14654    482    329 336713   9308   1553   1616  44343]\n",
      " [327192  87623   3077   3376 336713  40155  21560  17113 145864 327192]\n",
      " [    20   2190  34230   2893   2842  27731  14251   3129    435 327192]\n",
      " [    40 327192  93968    685 336713   1560  65361     46     18  12047]\n",
      " [  3078 107328    307 164107   5739   9919   8783   2782   2307 327192]\n",
      " [   897 327192  11478 336713   2165     28   9045  34230   3703 135720]\n",
      " [ 96352  34200 336713   1897   6586 164107  12651   1469    296 327192]\n",
      " [ 96352  65361 163009    357  15143 164107  14069   3376  44039 327192]\n",
      " [ 44039  42603  87623  20276 135720   6974 163009  44039  44125    473]]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_ints, seq_length=seq_length)\n",
    "print(len(reviews_ints[0]))\n",
    "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "print(features[:30,-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备好数据后，我们需要将数据拆分为训练集、验证集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 200)\n",
      "(2500, 200)\n",
      "(2500, 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "#训练集、验证集、测试集分割索引\n",
    "train_id = int(len(features)*split_frac)\n",
    "valid_id = (train_id+len(features))//2\n",
    "\n",
    "#encoded_labels = np.array(encoded_labels)\n",
    "\n",
    "train_x , train_y = features[:train_id] , encoded_labels[:train_id]\n",
    "valid_x , valid_y = features[train_id:valid_id] , encoded_labels[train_id:valid_id]\n",
    "test_x  , test_y  = features[valid_id:] , encoded_labels[valid_id:]\n",
    "\n",
    "\n",
    "print(train_x.shape)\n",
    "print(valid_x.shape)\n",
    "print(test_x.shape)\n",
    "type(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader 和批处理\n",
    "- 创建训练集、测试集和验证集后，再创建 DataLoader： 1.使用 TensorDataset 创建一种已知数据格式。TensorDataset 的参数包括输入数据集和目标数据集，并且第一个维度一样，然后创建一个数据集。 2.创建 DataLoader 并批处理训练、验证和测试张量数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[ 76000,   1484,  48208,  ...,  40155,  12047,  48208],\n",
      "        [     0,      0,      0,  ...,   9163,    237,   6890],\n",
      "        [     0,      0,      0,  ...,  16803,  34230,    158],\n",
      "        ...,\n",
      "        [ 87623,    296, 336713,  ...,    213,   8364,   5134],\n",
      "        [     0,      0,      0,  ..., 101872,    279,    169],\n",
      "        [     0,      0,      0,  ..., 164107,   4007, 327192]],\n",
      "       dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层级结构： \n",
    "- 1.一个嵌入层：将字词标记（整数）转换为特定大小的嵌入。 \n",
    "- 2.一个 LSTM 层级：由 hidden_state 大小和层级数量定义。\n",
    "- 3.一个全连接输出层：将 LSTM 层级输出映射到期望的 output_size。\n",
    "- 4.一个 S 型激活层：将所有输出转换为 0-1 之间的值；仅返回最后一个 S 型函数输出值作为网络的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    #文本情感分析\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        #初始化模型\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # 定义所有层级\n",
    "        self.embed = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim,n_layers,drop_prob,batch_first=True)\n",
    "        self.dropput = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim,output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        #前馈\n",
    "        batch_size = x.shape[0]\n",
    "        # 输出sigmoid值和隐藏层状态（下一个时间步利用）\n",
    "        print(x.shape)\n",
    "        x = self.embed(x)\n",
    "        lstm_out,hidden = self.lstm(x,hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous.view(-1,hidden_dim)\n",
    "        out = self.fc(lstm_out)\n",
    "        out = out.view(batch_size,-1)\n",
    "        out = self.dropput(out)\n",
    "        sig_out = self.sigmoid(out)\n",
    "        sig_out = sig_out[:,-1]\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # 为隐藏层状态和LSTM的结点状态初始化\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化网络，定义超参数。\n",
    "- vocab_size：词汇表的大小或输入（字词标记）的值范围。\n",
    "- output_size：期望输出的大小；希望输出的类别分数数量（正面/负面）。\n",
    "- embedding_dim：嵌入查询表的列数；嵌入大小。\n",
    "- hidden_dim：隐藏层的 LSTM 单元数量。\n",
    "- n_layers：网络中的 LSTM 层级数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74073, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab_to_int)\n",
    "output_size = 2\n",
    "embedding_dim = 200\n",
    "hidden_dim = 256\n",
    "n_layers = 3\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数为BCELoss，即二元交叉熵损失，对在 0-1 之间的单个值应用交叉熵损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # 初始化隐藏层\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # 加载器取数据\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        #在整个训练历史中反向传播\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # 将累计的梯度清零\n",
    "        net.zero_grad()\n",
    "        print(inputs.shape)\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # 计算损失，执行反向传播\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "# 初始化隐藏层\n",
    "h = net.init_hidden(batch_size)\n",
    "#进入测试模式\n",
    "net.eval()\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    \n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    \n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    \n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # 根据输出概率判断类别\n",
    "    pred = torch.round(output.squeeze())\n",
    "    \n",
    "    # 预测标签与实际标签对比\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "#测试损失求平均\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# 测试数据上的精度\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
